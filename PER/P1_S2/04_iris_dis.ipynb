{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris con clasificadores discriminativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que árboles y clasificadores generativos, es muy fácil aprender y evaluar clasificadores discriminativos con sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura del corpus iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, shuffle=True, random_state=23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo Perceptrón es una técnica pionera del ML, ampliamente conocida. En la actualidad se tiende a ver como un caso particular de descenso por gradiente estocástico para aprendizaje de clasificadores lineales. Éste es, precisamente, el punto de vista en sklearn. Destacaremos aquí que el algoritmo baraja los datos tras cada iteración a no ser que se indique lo contrario (shuffle=False); también que puede añadirse una penalización (penalty={'l2','l1','elasticnet'}, default=None) cuya fuerza se regula mediante un parámetro alpha (0.0001 por omisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de Perceptron() es 76.7%\n"
     ]
    }
   ],
   "source": [
    "clf = Perceptron(random_state=0).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Estudia el efecto de alpha sobre la precisión de Perceptrón con penalización 'l2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de Perceptron(alpha=1e-06, penalty='l1') es 76.7%\n",
      "La precisión de Perceptron(alpha=1e-05, penalty='l1') es 76.7%\n",
      "La precisión de Perceptron(penalty='l1') es 66.7%\n",
      "La precisión de Perceptron(alpha=0.001, penalty='l1') es 91.7%\n",
      "La precisión de Perceptron(alpha=0.01, penalty='l1') es 35.0%\n",
      "La precisión de Perceptron(alpha=0.1, penalty='l1') es 61.7%\n",
      "La precisión de Perceptron(alpha=0, penalty='l1') es 76.7%\n",
      "La precisión de Perceptron(alpha=1e-06, penalty='l2') es 76.7%\n",
      "La precisión de Perceptron(alpha=1e-05, penalty='l2') es 95.0%\n",
      "La precisión de Perceptron(penalty='l2') es 36.7%\n",
      "La precisión de Perceptron(alpha=0.001, penalty='l2') es 65.0%\n",
      "La precisión de Perceptron(alpha=0.01, penalty='l2') es 70.0%\n",
      "La precisión de Perceptron(alpha=0.1, penalty='l2') es 30.0%\n",
      "La precisión de Perceptron(alpha=0, penalty='l2') es 76.7%\n",
      "La precisión de Perceptron(alpha=1e-06, penalty='elasticnet') es 76.7%\n",
      "La precisión de Perceptron(alpha=1e-05, penalty='elasticnet') es 95.0%\n",
      "La precisión de Perceptron(penalty='elasticnet') es 63.3%\n",
      "La precisión de Perceptron(alpha=0.001, penalty='elasticnet') es 73.3%\n",
      "La precisión de Perceptron(alpha=0.01, penalty='elasticnet') es 66.7%\n",
      "La precisión de Perceptron(alpha=0.1, penalty='elasticnet') es 70.0%\n",
      "La precisión de Perceptron(alpha=0, penalty='elasticnet') es 76.7%\n"
     ]
    }
   ],
   "source": [
    "for l in ['l1', 'l2', 'elasticnet']:\n",
    "    for al in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0]:\n",
    "        # para la elasticnet existe el parametro l1_ratio que indica el ratio de l1 que se coge para la muestra, default = 0.15\n",
    "        clf = Perceptron(penalty=l, alpha= al, random_state=0).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión logística es un modelo de clasificación lineal muy importante pues puede verse como el primer paso en la construcción de redes neuronales multi-capa para clasificación. Además, si nuestro objetivo es entrenar un clasificador lineal tan preciso como sea posible, regresión logística es el primer camino a explorar. La función LogisticRegression de sklearn incluye diferentes solvers (algoritmos de optimización) con recomendaciones sobre cómo escoger el más adecuado para la tarea concreta que nos ocupa. Por ejemplo, para conjuntos de datos pequeños se recomienda usar 'liblinear'. Todos los solvers pueden aplicarse con regularización L2 (opción por omisión) o sin ella (penalty='None')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de LogisticRegression(solver='liblinear') es 96.7%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Estudia el efecto del solver escogido (con max_iter=10000) sobre la precisión de regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:420: ConvergenceWarning: Newton solver did not converge after 1 iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:420: ConvergenceWarning: Newton solver did not converge after 1 iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:420: ConvergenceWarning: Newton solver did not converge after 1 iterations.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de LogisticRegression(max_iter=1) es 35.0%\n",
      "La precisión de LogisticRegression(max_iter=10) es 98.3%\n",
      "La precisión de LogisticRegression() es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1000) es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=10000) es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=100000) es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1, solver='liblinear') es 73.3%\n",
      "La precisión de LogisticRegression(max_iter=10, solver='liblinear') es 96.7%\n",
      "La precisión de LogisticRegression(solver='liblinear') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=1000, solver='liblinear') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=10000, solver='liblinear') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=100000, solver='liblinear') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=1, solver='newton-cg') es 73.3%\n",
      "La precisión de LogisticRegression(max_iter=10, solver='newton-cg') es 96.7%\n",
      "La precisión de LogisticRegression(solver='newton-cg') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1000, solver='newton-cg') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=10000, solver='newton-cg') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=100000, solver='newton-cg') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1, solver='newton-cholesky') es 85.0%\n",
      "La precisión de LogisticRegression(max_iter=10, solver='newton-cholesky') es 95.0%\n",
      "La precisión de LogisticRegression(solver='newton-cholesky') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1000, solver='newton-cholesky') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=10000, solver='newton-cholesky') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=100000, solver='newton-cholesky') es 95.0%\n",
      "La precisión de LogisticRegression(max_iter=1, solver='sag') es 65.0%\n",
      "La precisión de LogisticRegression(max_iter=10, solver='sag') es 98.3%\n",
      "La precisión de LogisticRegression(solver='sag') es 98.3%\n",
      "La precisión de LogisticRegression(max_iter=1000, solver='sag') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=10000, solver='sag') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=100000, solver='sag') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=1, solver='saga') es 65.0%\n",
      "La precisión de LogisticRegression(max_iter=10, solver='saga') es 96.7%\n",
      "La precisión de LogisticRegression(solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(max_iter=1000, solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(max_iter=10000, solver='saga') es 96.7%\n",
      "La precisión de LogisticRegression(max_iter=100000, solver='saga') es 96.7%\n"
     ]
    }
   ],
   "source": [
    "for solv in ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']:\n",
    "        for mI in [1, 10, 100, 1000, 10000, 100000]:\n",
    "                clf = LogisticRegression(solver=solv, max_iter=mI).fit(X_train, y_train)\n",
    "                acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "                print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de LogisticRegression(C=0.1, solver='saga') es 100.0%\n",
      "La precisión de LogisticRegression(C=1, solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(C=10, solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(C=100, solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(C=1000, solver='saga') es 98.3%\n",
      "La precisión de LogisticRegression(C=10000, solver='saga') es 98.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for c in [0.1, 1, 10, 100, 1000, 10000]:\n",
    "        clf = LogisticRegression(solver='saga', C = c).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que los clasificadores discriminativos en general, regresión logística es flexible en preproceso de características. Por ejemplo, aparte de las $4$ características originales, $5$ en notación homogénea, podemos añadir $4*5/2=10$ características cuadráticas y obtener $D=10+5=15$ características en total. Una manera sencilla de implementar este preproceso en sklearn consiste en crear un pipeline que preceda el clasificador con los pasos de preproceso que consideremos oportunos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('logisticregression', LogisticRegression(max_iter=10000))]) es 98.3%\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(PolynomialFeatures(degree=2),\n",
    "    LogisticRegression(max_iter=10000)).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
