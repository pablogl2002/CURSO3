{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST con GradientBoosting\n",
    "En esta segunda sesión práctica vamos a entrenar clasificadores basados en GradientBoosting.\n",
    "Claramente estos clasificadores son más costosos de entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)\n",
    "\n",
    "data = mnist.data\n",
    "targets = mnist.target \n",
    "\n",
    "targets=targets.to_numpy()\n",
    "targets=np.int8(targets)\n",
    "\n",
    "data=data.to_numpy()\n",
    "data=np.float32(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos\n",
    "\n",
    "Vamos a partir los datos en tres conjuntos: training, validation y test. Con un 80%, 10% y 10% respectivamente. \n",
    "\n",
    "Emplearemos el conjunto de training para aprender los parámetros del modelos, el conjunto de validation para escoger los mejores hiperparámetros. Finalmente reportaremos el resultado final sobre el conjunto de test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1**    \n",
    "\n",
    "Realiza la partición de los datos en las particiones definidas (80%,10%,10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de los datos\n",
    "tam_test = 0.1\n",
    "tam_val = 0.1\n",
    "tam_train = 0.8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=tam_test, random_state=23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=tam_val/(tam_train + tam_test), random_state=23) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 2**   \n",
    "\n",
    "Prueba un clasificador GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GradientBoosting\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podrás comprobar como es un clasificador mucho más lento. \n",
    "\n",
    "### **Ejercicio 3**   \n",
    "\n",
    "Para reducir el coste computacional se propone crear un pipeline donde se reduzca el número de características mediante PCA. En concreto el número de componentes (dimensiones) a las que reducimos con PCA es un hyperparámetro que tendrás que estimar con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mejorar velocidad reduciendo dimensionalidad con PCA\n",
    "for n_components in [2, 4, 8, 16, 32]:\n",
    "        clf = make_pipeline(PCA(n_components=n_components) if n_components>0 else None,\n",
    "                GradientBoostingClassifier()).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De entre los diferentes parámeros que tiene el clasificador GradientBoosting de sklearn, cabría destacar:\n",
    "\n",
    "**learning_rate**\n",
    "\n",
    "**n_estimators**\n",
    "\n",
    "**min_samples_split**\n",
    "\n",
    "**max_features** \n",
    "\n",
    "\n",
    "Para más información leer la documentación en sklearn.\n",
    "\n",
    "Alguno de estos parámetros influyen considerablemente en la velocidad de optimización. Por ejemplo **max_features** y **min_samples_split** entre otros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 4**   \n",
    "\n",
    "Se propone variar alguno de estos parámetros para ver si se obtiene una similar tasa de acierto pero con mejor velocidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular tiempo y tasa de acierto de la versión original con PCA\n",
    "ini = time.time()\n",
    "clf = make_pipeline(PCA(n_components=16),\n",
    "            GradientBoostingClassifier()).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "fin = time.time()\n",
    "print('La precisión de {0!s} es {1:.1%} y tarda {2:.5}'.format(clf, acc, fin - ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular tiempo y tasa de acierto modificando alguno de los parámetros propuestos\n",
    "ini = time.time()\n",
    "clf = make_pipeline(PCA(n_components=16),\n",
    "            GradientBoostingClassifier(max_features=20, n_stimators=100, min_samples_split=10)).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "fin = time.time()\n",
    "print('La precisión de {0!s} es {1:.1%} y tarda {2:.5}'.format(clf, acc, fin - ini))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente sklearn propone otro tipo de algoritmo de GradientBoosting que soporta paralelismo con OMP además de otras mejoras computacionales basadas en la discretización de las componentes mediante un histograma: el HistGradientBoostingClassifier. Su tiempo de ejecución es mucho mejor. Además se pueden obtener mejores resultados.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 5**   \n",
    "\n",
    "Pruébalo y compara los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar HistGradientBoostingClassifier \n",
    "clf = make_pipeline(PCA(n_components=16),\n",
    "            HistGradientBoostingClassifier()).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 6**   \n",
    "\n",
    "Prueba los parámetros del HistGradientBoostingClassifier que mejoren la tasa de acierto. En cualquier caso la selección de estos parámetros debe seguir el protocolo de experimentación antes expuesto. Esto es, escoger el mejor parámetro con datos de validación y reportar resultados con los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = -1\n",
    "bestN_comp = -1\n",
    "for n_component in [8, 16, 32]:\n",
    "    clf = make_pipeline(PCA(n_components=n_component),\n",
    "                HistGradientBoostingClassifier()).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "    if acc > max:\n",
    "        bestN_comp = n_component\n",
    "        max = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=32, max_depth=8))]) es 95.0% y tarda 31.902\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=64, max_depth=8))]) es 94.8% y tarda 31.771\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=128, max_depth=8))]) es 95.2% y tarda 31.609\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=32, max_depth=16))]) es 95.0% y tarda 31.998\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=64, max_depth=16))]) es 94.9% y tarda 32.123\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=128, max_depth=16))]) es 94.9% y tarda 32.127\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=32, max_depth=32))]) es 94.9% y tarda 31.902\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=64, max_depth=32))]) es 95.1% y tarda 31.984\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('histgradientboostingclassifier',\n",
      "                 HistGradientBoostingClassifier(max_bins=128, max_depth=32))]) es 95.1% y tarda 32.353\n"
     ]
    }
   ],
   "source": [
    "bestN_comp=16\n",
    "for md in [8, 16, 32]:\n",
    "    for mB in [32, 64, 128]:\n",
    "        ini = time.time()\n",
    "        clf = make_pipeline(PCA(n_components=bestN_comp),\n",
    "                    HistGradientBoostingClassifier(max_depth=md, max_bins=mB)).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        fin = time.time()\n",
    "        print('La precisión de {0!s} es {1:.1%} y tarda {2:.5}'.format(clf, acc, fin - ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minL in [2, 3, 4, 5]:\n",
    "        clf = make_pipeline(PCA(n_components=32),\n",
    "                        HistGradientBoostingClassifier(min_samples_leaf=minL))\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        print(clf, acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline(steps=[('pca', PCA(n_components=32)),\n",
    "                ('histgradientboostingclassifier',\n",
    "                 HistGradientBoostingClassifier(min_samples_leaf=2))]) 0.9604342236823311\n",
    "Pipeline(steps=[('pca', PCA(n_components=32)),\n",
    "                ('histgradientboostingclassifier',\n",
    "                 HistGradientBoostingClassifier(min_samples_leaf=3))]) 0.9587201828310241\n",
    "Pipeline(steps=[('pca', PCA(n_components=32)),\n",
    "                ('histgradientboostingclassifier',\n",
    "                 HistGradientBoostingClassifier(min_samples_leaf=4))]) 0.9628624482216827\n",
    "Pipeline(steps=[('pca', PCA(n_components=32)),\n",
    "                ('histgradientboostingclassifier',\n",
    "                 HistGradientBoostingClassifier(min_samples_leaf=5))]) 0.9614340808455935"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
