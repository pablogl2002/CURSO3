{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST \n",
    "\n",
    "MNIST es un dataset clásico de clasificación de dígitos manuscritos (0..9). Los dígitos están representados mediante imágenes de 28x28 píxels en escala de grises. Los mejores resultados en este dataset se consiguen mediante redes neuronales convolucionales. Sin embargo nosotros vamos a emplear otro tipos de clasificadores vistos en clase. En concreto aquellos basados en árboles y distancias.\n",
    "\n",
    "Por otro lado vamos a realizar un diseño del experimento de manera adecuada empleando conjuntos de validación para poder estimar los mejores hiperparámetros. El conjunto de datos de test sólo lo emplearemos para realizar la última clasificación y reportar resultados.\n",
    "\n",
    "En esta primera práctica vamos a entrenar clasificadores basados en árboles de decisión sobre este dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre vamos a echar un vistazo a la descripción de este dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n",
      "**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n",
      "**Please cite**:  \n",
      "\n",
      "The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n",
      "\n",
      "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n",
      "\n",
      "With some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n",
      "\n",
      "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los campos \"data\" y \"target\" se leen por defecto como una tabla de pandas. Dejar los datos en pandas puede tener alguna ventaja como por ejemplo que las características tienen un nombre asociado. No sería un problema continuar con llamadas a funciones sklearn con estos tipos de datos, pero si queremos aplicar alguna transformación con numpy lo ideal es convertirlos a numpy. Convertimos los datos y targets a numpy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)\n",
    "\n",
    "data = mnist.data\n",
    "targets = mnist.target \n",
    "\n",
    "print(type(data))\n",
    "print(type(targets))\n",
    "\n",
    "#-------\n",
    "\n",
    "targets=targets.to_numpy()\n",
    "targets=np.int8(targets)\n",
    "\n",
    "data=data.to_numpy()\n",
    "data=np.float32(data)\n",
    "\n",
    "print(type(data))\n",
    "print(type(targets))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos por ejemplo la imagen promedio de algunas clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f17b64d8c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeAElEQVR4nO3dX2zV9f3H8dehtIe2nB7E0p5TKU3nYBowJP4DiX/AxMYmI0O2BDVZ4MboBBJSjRnjwmYX1LhIvGCyzCxMMpncqDORiF2wZYaxIMNI8M9wFCnaplChpxQ4pe3nd8GPk1X+fj6c03dP+3wkJ6HnnBfn02+/7avfnvN9n4hzzgkAAAMTrBcAABi/KCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYmWi9gB8aGhrSd999p1gspkgkYr0cAIAn55x6e3tVVVWlCROufqwz6krou+++U3V1tfUyAAA3qL29XdOnT7/qfUZdCcViMUkXFl9WVma8GgCAr1Qqperq6szP86vJWQm99tpr+t3vfqeOjg7Nnj1br776qh544IFr5i7+Ca6srIwSAoA8dj1PqeTkhQnbtm3TmjVrtG7dOu3fv18PPPCA6uvrdfTo0Vw8HAAgT0VyMUV73rx5uvPOO7Vp06bMdbfffruWLFmipqamq2ZTqZTi8bh6eno4EgKAPOTzczzrR0L9/f3at2+f6urqhl1fV1en3bt3X3L/dDqtVCo17AIAGB+yXkInTpzQ4OCgKisrh11fWVmpzs7OS+7f1NSkeDyeufDKOAAYP3J2suoPn5Byzl32Saq1a9eqp6cnc2lvb8/VkgAAo0zWXx1XXl6ugoKCS456urq6Ljk6kqRoNKpoNJrtZQAA8kDWj4SKiop01113qbm5edj1zc3NWrBgQbYfDgCQx3JynlBDQ4N++ctf6u6779Z9992nP/7xjzp69KieeeaZXDwcACBP5aSEli1bpu7ubv32t79VR0eH5syZo+3bt6umpiYXDwcAyFM5OU/oRnCeEADkN9PzhAAAuF6UEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzEy0XgDsOedGNDdaH2e0i0QiY+6xRvJzwujEkRAAwAwlBAAwk/USamxsVCQSGXZJJBLZfhgAwBiQk+eEZs+erb///e+ZjwsKCnLxMACAPJeTEpo4cSJHPwCAa8rJc0KHDh1SVVWVamtr9fjjj+vw4cNXvG86nVYqlRp2AQCMD1kvoXnz5mnLli3asWOHXn/9dXV2dmrBggXq7u6+7P2bmpoUj8czl+rq6mwvCQAwSkVcjk/C6Ovr06233qoXXnhBDQ0Nl9yeTqeVTqczH6dSKVVXV6unp0dlZWW5XBr+H+cJ5QfOE0K+SKVSisfj1/VzPOcnq5aWluqOO+7QoUOHLnt7NBpVNBrN9TIAAKNQzs8TSqfT+uKLL5RMJnP9UACAPJP1Enr++efV2tqqtrY2/etf/9IvfvELpVIpLV++PNsPBQDIc1n/c9yxY8f0xBNP6MSJE5o2bZrmz5+vPXv2qKamJtsPBQDIc1kvobfeeivb/+WYEPLk+uDg4Ihkzp8/752RpP7+/lGbGRgY8M5I0tDQkHdmNL9wYsKEsD92hJxgXlRU5J0JeT445HFCMtKFcx59hWzz8fwCDWbHAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMJPzN7UbzUIHT4YMuQwZwnnmzBnvzKlTp7wz33//vXdG0hXfsv1qTpw44Z05efKkd6anp8c7I4Vt8/99Z+DrFbIPhQynDd3HCwsLvTM33XSTdybkfcZCJvLPmDHDOyNJ06ZN886UlpZ6Z0IGrI6VoaccCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzIzrKdohk4ylsKnJvb293pnjx497Z7799lvvTFtbm3dGkg4fPjwij3Xs2DHvTMiEbyns6xQyIT1k3xupjCRNnOj/o6GystI7c/vtt3tn7r33Xu9M6HYImSYekgnZ3gUFBd6Z0YgjIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGbGzABT55x3JnSoYcjAyjNnznhnTp065Z0ZqaGnknTkyBHvzH//+1/vTFdXl3emr6/POxOqqKjIOzNhgv/vfwMDA96Z0O1w+vRp78y5c+e8M8XFxd6Zmpoa70zIYFpJOn/+fFAO148jIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGbGzADTECFDT0fSxIn+X57CwkLvTElJiXdGkqZMmeKdmT59unfm5ptv9s6EbAcp7HOKx+PemZChpyFDcL/55hvvjCR9+eWX3pmenh7vTMj3YMjw19D9IeR7sKCgwDsTiUS8M2MFR0IAADOUEADAjHcJ7dq1S4sXL1ZVVZUikYjefffdYbc759TY2KiqqioVFxdr4cKFOnjwYLbWCwAYQ7xLqK+vT3PnztXGjRsve/vLL7+sDRs2aOPGjdq7d68SiYQeeeSR4DeVAgCMXd7PutXX16u+vv6ytznn9Oqrr2rdunVaunSpJOmNN95QZWWltm7dqqeffvrGVgsAGFOy+pxQW1ubOjs7VVdXl7kuGo3qoYce0u7duy+bSafTSqVSwy4AgPEhqyXU2dkpSaqsrBx2fWVlZea2H2pqalI8Hs9cqqurs7kkAMAolpNXx/3wNe/OuSu+Dn7t2rXq6enJXNrb23OxJADAKJTVk1UTiYSkC0dEyWQyc31XV9clR0cXRaNRRaPRbC4DAJAnsnokVFtbq0Qioebm5sx1/f39am1t1YIFC7L5UACAMcD7SOj06dP6+uuvMx+3tbXp008/1dSpUzVjxgytWbNG69ev18yZMzVz5kytX79eJSUlevLJJ7O6cABA/vMuoU8++USLFi3KfNzQ0CBJWr58uf785z/rhRde0NmzZ/Xss8/q5MmTmjdvnj788EPFYrHsrRoAMCZ4l9DChQuvOnQwEomosbFRjY2NN7IubyEDAEMGIUphwxCLi4u9M2VlZd6ZadOmeWf6+/u9M1LYoMYrPTd4NSHDPkO2gxS2vpCvU8i+d+LECe/Mnj17vDOSdPz4ce9MOp32zkyePNk7EzIwNiQjhQ33DRl6GvqzaCwYv585AMAcJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMBMVt9ZNd+MxSnaAwMD3pmhoSHvjKSgd8QNeayQCcgh07Alqby83DszadIk70zIxOkQV5t4fzXnzp3zzoRMVb/55pu9M//7rs3XK+TrKoVN0Q7ZDuMZR0IAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMjOsBppFIJCgXMqBwpIaexmIx78zg4KB3Rgob3Bmy7aZMmeKdCRmMKUmlpaXemZD9KGTQ7MmTJ70zR44c8c5I0vfff++dCRk0e8stt3hnqqqqvDMhw4ClsO9b+OFICABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBkGmI5QbuJE/00dMjyxpKTEO+Oc886EPlbI5xQyVDRkuKoU9rU9f/68d6a7u9s78/nnn3tnvvzyS++MJPX19XlnamtrvTPV1dXemfLycu/MSO4PIUK+B0dqbbnGkRAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAz43qAaaiQwYEFBQXemZBhn0NDQ96Z0EGII/VYIdtuYGDAOyNJ/f393plUKuWd+c9//uOd+fe//+2d+fbbb70zUti+FzKM9JZbbvHOTJ482TsTsg+FCh0IPF5xJAQAMEMJAQDMeJfQrl27tHjxYlVVVSkSiejdd98ddvuKFSsUiUSGXebPn5+t9QIAxhDvEurr69PcuXO1cePGK97n0UcfVUdHR+ayffv2G1okAGBs8n5hQn19verr6696n2g0qkQiEbwoAMD4kJPnhFpaWlRRUaFZs2bpqaeeUldX1xXvm06nlUqlhl0AAOND1kuovr5eb775pnbu3KlXXnlFe/fu1cMPP6x0On3Z+zc1NSkej2cuIS/zBADkp6yfJ7Rs2bLMv+fMmaO7775bNTU1ev/997V06dJL7r927Vo1NDRkPk6lUhQRAIwTOT9ZNZlMqqamRocOHbrs7dFoVNFoNNfLAACMQjk/T6i7u1vt7e1KJpO5figAQJ7xPhI6ffq0vv7668zHbW1t+vTTTzV16lRNnTpVjY2N+vnPf65kMqkjR47oN7/5jcrLy/XYY49ldeEAgPznXUKffPKJFi1alPn44vM5y5cv16ZNm3TgwAFt2bJFp06dUjKZ1KJFi7Rt2zbFYrHsrRoAMCZ4l9DChQuvOqBvx44dN7SgfBAyhDMkM3Gi/1N2IcMTJ0wI+6tsyADTwcHBEXmckEGkknTmzBnvzLFjx7wz+/fv98588cUX3pnQ7TBr1izvzMyZM70zIX+mD/mFNnSA6Uh9r49nzI4DAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJjJ+Tur4oKQyboh060LCwtH5HGksInYIdthYGDAOxM6Pbq3t9c7c/jwYe/MwYMHvTPff/+9d+amm27yzkjS7bff7p35yU9+4p1JJBLemZKSEu9MUVGRd0YKm77NFG0/HAkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwwwDTETJSA0xDHid04OJIPZZzzjsTMvRUkrq7u70zX3/9tXemo6PDOxMynPbHP/6xd0aSZs+e7Z2ZMWOGd2bKlCnemeLiYu9MyLaTRm6A6XgeesqREADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADMMMB3FRmqoYcig1FAhw0gHBwe9M729vd4ZSWpvb/fOfPvtt96ZdDrtnUkmk96Z2267zTsjSbfeeqt3Ztq0ad6ZkpIS70zIMNKQQaQSw0hHAkdCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzDDAdIwZyeGJIcNIBwYGvDMhw0g7Ojq8M1LYANOTJ096Z0pLS70zIQNMf/SjH3lnJKmystI7M3nyZO9MUVGRdyZk4G7o9wXDSHOPIyEAgBlKCABgxquEmpqadM899ygWi6miokJLlizRV199New+zjk1NjaqqqpKxcXFWrhwoQ4ePJjVRQMAxgavEmptbdXKlSu1Z88eNTc3a2BgQHV1derr68vc5+WXX9aGDRu0ceNG7d27V4lEQo888kjwm4wBAMYurxcmfPDBB8M+3rx5syoqKrRv3z49+OCDcs7p1Vdf1bp167R06VJJ0htvvKHKykpt3bpVTz/9dPZWDgDIezf0nFBPT48kaerUqZKktrY2dXZ2qq6uLnOfaDSqhx56SLt3777s/5FOp5VKpYZdAADjQ3AJOefU0NCg+++/X3PmzJEkdXZ2Srr05Z2VlZWZ236oqalJ8Xg8c6murg5dEgAgzwSX0KpVq/TZZ5/pr3/96yW3/fC19c65K77efu3aterp6clcQs7TAADkp6CTVVevXq333ntPu3bt0vTp0zPXJxIJSReOiP73xLqurq4rnvwWjUYVjUZDlgEAyHNeR0LOOa1atUpvv/22du7cqdra2mG319bWKpFIqLm5OXNdf3+/WltbtWDBguysGAAwZngdCa1cuVJbt27V3/72N8VisczzPPF4XMXFxYpEIlqzZo3Wr1+vmTNnaubMmVq/fr1KSkr05JNP5uQTAADkL68S2rRpkyRp4cKFw67fvHmzVqxYIUl64YUXdPbsWT377LM6efKk5s2bpw8//FCxWCwrCwYAjB1eJXQ9AysjkYgaGxvV2NgYuibcgJChokNDQ0GPdf78ee9MyEnLx48f984cO3bMOyNJJ06c8M6EDLmcNm2adybklaMXn6f1VVZW5p0JeW63oKDAO8NQ0bGF2XEAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADNB76yKkTFSE7FDpmFL0pkzZ7wz3d3d3pmOjg7vTMjkbUlKp9PemdLSUu/M5MmTvTMhE7GnTJninZGkSZMmeWcmTvT/cRIyEZsp2mMLR0IAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMMMB0FAsZYDo4OOidOXfunHdGknp7e70zp06d8s6cPHnSO3P27FnvjBQ2hLOsrMw7c/PNN3tnpk6d6p0JGa4qSUVFRd6ZCRP8f6dlGCk4EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGAaYjJGQYaUhmYGDAO5NOp70zktTX1zcimf7+fu9MQUGBd0aSYrGYdyZk6GnIMNKQQanFxcXeGSnscwoZYAqw1wAAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADDDANMREolErJcwKhQWFnpnQoaKDg0NeWck6fz5896ZkM8pHo97Z0KGnk6ePNk7I0nRaNQ7EzLAlO8LcCQEADBDCQEAzHiVUFNTk+655x7FYjFVVFRoyZIl+uqrr4bdZ8WKFYpEIsMu8+fPz+qiAQBjg1cJtba2auXKldqzZ4+am5s1MDCgurq6S96o7NFHH1VHR0fmsn379qwuGgAwNni9MOGDDz4Y9vHmzZtVUVGhffv26cEHH8xcH41GlUgksrNCAMCYdUPPCfX09Ei69FU7LS0tqqio0KxZs/TUU0+pq6vriv9HOp1WKpUadgEAjA/BJeScU0NDg+6//37NmTMnc319fb3efPNN7dy5U6+88or27t2rhx9+WOl0+rL/T1NTk+LxeOZSXV0duiQAQJ6JOOdcSHDlypV6//339fHHH2v69OlXvF9HR4dqamr01ltvaenSpZfcnk6nhxVUKpVSdXW1enp6VFZWFrK0MWNwcNA7c6Wyv5re3l7vjCQdP37cO3O1o+JsZi4epfsazecJlZeXe2cqKiq8M1LYOUkh53NNmjTJOzNxov/pjSHnMEmcxxQqlUopHo9f18/xoJNVV69erffee0+7du26agFJUjKZVE1NjQ4dOnTZ26PRaNCJcQCA/OdVQs45rV69Wu+8845aWlpUW1t7zUx3d7fa29uVTCaDFwkAGJu8jlFXrlypv/zlL9q6datisZg6OzvV2dmps2fPSpJOnz6t559/Xv/85z915MgRtbS0aPHixSovL9djjz2Wk08AAJC/vI6ENm3aJElauHDhsOs3b96sFStWqKCgQAcOHNCWLVt06tQpJZNJLVq0SNu2bQv6ezEAYGzz/nPc1RQXF2vHjh03tCAAwPjBFO1RLOQVPSGv1CotLfXOhCopKfHOhLwq7OKfiH2FvCIx5OsU8mKckK9T6Nc25OtUVFTknSkoKPDOMK17bGGAKQDADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADMMMB3FQoYuhgwwDclIYcMxQ95NfqQyI/lYoevzFfq21iM1JHSkMhi9OBICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgJlRNzvu4kytVCplvBJcy2ies8bsuAuYHQcLF39+X89+PupKqLe3V5JUXV1tvBIAwI3o7e1VPB6/6n0ibqR+JbtOQ0ND+u677xSLxS75jSeVSqm6ulrt7e0qKyszWqE9tsMFbIcL2A4XsB0uGA3bwTmn3t5eVVVVXfOoetQdCU2YMEHTp0+/6n3KysrG9U52EdvhArbDBWyHC9gOF1hvh2sdAV3ECxMAAGYoIQCAmbwqoWg0qhdffFHRaNR6KabYDhewHS5gO1zAdrgg37bDqHthAgBg/MirIyEAwNhCCQEAzFBCAAAzlBAAwExeldBrr72m2tpaTZo0SXfddZf+8Y9/WC9pRDU2NioSiQy7JBIJ62Xl3K5du7R48WJVVVUpEono3XffHXa7c06NjY2qqqpScXGxFi5cqIMHD9osNoeutR1WrFhxyf4xf/58m8XmSFNTk+655x7FYjFVVFRoyZIl+uqrr4bdZzzsD9ezHfJlf8ibEtq2bZvWrFmjdevWaf/+/XrggQdUX1+vo0ePWi9tRM2ePVsdHR2Zy4EDB6yXlHN9fX2aO3euNm7ceNnbX375ZW3YsEEbN27U3r17lUgk9Mgjj2TmEI4V19oOkvToo48O2z+2b98+givMvdbWVq1cuVJ79uxRc3OzBgYGVFdXp76+vsx9xsP+cD3bQcqT/cHliXvvvdc988wzw6677bbb3K9//WujFY28F1980c2dO9d6GaYkuXfeeSfz8dDQkEskEu6ll17KXHfu3DkXj8fdH/7wB4MVjowfbgfnnFu+fLn72c9+ZrIeK11dXU6Sa21tdc6N3/3hh9vBufzZH/LiSKi/v1/79u1TXV3dsOvr6uq0e/duo1XZOHTokKqqqlRbW6vHH39chw8ftl6Sqba2NnV2dg7bN6LRqB566KFxt29IUktLiyoqKjRr1iw99dRT6urqsl5STvX09EiSpk6dKmn87g8/3A4X5cP+kBcldOLECQ0ODqqysnLY9ZWVlers7DRa1cibN2+etmzZoh07duj1119XZ2enFixYoO7ubuulmbn49R/v+4Yk1dfX680339TOnTv1yiuvaO/evXr44YeVTqetl5YTzjk1NDTo/vvv15w5cySNz/3hcttByp/9YdRN0b6aH761g3NuXL3BVX19febfd9xxh+677z7deuuteuONN9TQ0GC4Mnvjfd+QpGXLlmX+PWfOHN19992qqanR+++/r6VLlxquLDdWrVqlzz77TB9//PElt42n/eFK2yFf9oe8OBIqLy9XQUHBJb/JdHV1XfIbz3hSWlqqO+64Q4cOHbJeipmLrw5k37hUMplUTU3NmNw/Vq9erffee08fffTRsLd+GW/7w5W2w+WM1v0hL0qoqKhId911l5qbm4dd39zcrAULFhityl46ndYXX3yhZDJpvRQztbW1SiQSw/aN/v5+tba2jut9Q5K6u7vV3t4+pvYP55xWrVqlt99+Wzt37lRtbe2w28fL/nCt7XA5o3Z/MHxRhJe33nrLFRYWuj/96U/u888/d2vWrHGlpaXuyJEj1ksbMc8995xraWlxhw8fdnv27HE//elPXSwWG/PboLe31+3fv9/t37/fSXIbNmxw+/fvd998841zzrmXXnrJxeNx9/bbb7sDBw64J554wiWTSZdKpYxXnl1X2w69vb3uueeec7t373ZtbW3uo48+cvfdd5+75ZZbxtR2+NWvfuXi8bhraWlxHR0dmcuZM2cy9xkP+8O1tkM+7Q95U0LOOff73//e1dTUuKKiInfnnXcOeznieLBs2TKXTCZdYWGhq6qqckuXLnUHDx60XlbOffTRR07SJZfly5c75y68LPfFF190iUTCRaNR9+CDD7oDBw7YLjoHrrYdzpw54+rq6ty0adNcYWGhmzFjhlu+fLk7evSo9bKz6nKfvyS3efPmzH3Gw/5wre2QT/sDb+UAADCTF88JAQDGJkoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGb+D0soR8b8oG4JAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "clase=7\n",
    "m=np.mean(data[targets==clase],axis=0)\n",
    "m=np.reshape(m,(28,28))\n",
    "\n",
    "plt.imshow(m, cmap=plt.cm.gray_r, interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos\n",
    "\n",
    "Vamos a partir los datos en tres conjuntos: training, validation y test. Con un 80%, 10% y 10% respectivamente. \n",
    "\n",
    "Emplearemos el conjunto de training para aprender los parámetros del modelos, el conjunto de validation para escoger los mejores hiperparámetros. Finalmente reportaremos el resultado final sobre el conjunto de test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1**  \n",
    "\n",
    "Realiza la partición de los datos en las particiones definidas (80%,10%,10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55999, 784)\n"
     ]
    }
   ],
   "source": [
    "# Realizar partición de datos\n",
    "tam_test = 0.1\n",
    "tam_val = 0.1\n",
    "tam_train = 0.8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=tam_test, random_state=23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=tam_val/(tam_train + tam_test), random_state=23) \n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, shuffle=True, random_state=23)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True, random_state=23)\n",
    "'''\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 2**   \n",
    "\n",
    "Define un clasificador de tipo árbol de decisión. Varía el parámetro \"max_depth\" y quédate con aquel que mejor resultado se obtenga con los datos de validación. Finalmente obten el resultado sobre los datos de test con dicho mejor parámetro. Este último clasificador con el mejor parámetro se debería entrenar con todos los datos (training+validación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de DecisionTreeClassifier(max_depth=15, random_state=23) es 86.9%\n",
      "La precisión de DecisionTreeClassifier(max_depth=16, random_state=23) es 87.2%\n",
      "La precisión de DecisionTreeClassifier(max_depth=18, random_state=23) es 87.1%\n",
      "La precisión de DecisionTreeClassifier(max_depth=19, random_state=23) es 87.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m best \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m [\u001b[39m15\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m18\u001b[39m, \u001b[39m19\u001b[39m, \u001b[39m20\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     clf \u001b[39m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[39m=\u001b[39;49md, random_state\u001b[39m=\u001b[39;49m\u001b[39m23\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(y_val, clf\u001b[39m.\u001b[39mpredict(X_val))\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLa precisión de \u001b[39m\u001b[39m{0!s}\u001b[39;00m\u001b[39m es \u001b[39m\u001b[39m{1:.1%}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(clf, acc))\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Encontrar mejor parámetro\n",
    "#for d in [2, 4, 8, 16, 32, 64]:\n",
    "max = -1\n",
    "best = -1\n",
    "for d in [15, 16, 18, 19, 20]:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=23).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "    if acc > max:\n",
    "        best = d\n",
    "        max = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de DecisionTreeClassifier(max_depth=12, random_state=23) es 86.9%\n",
      "La precisión de DecisionTreeClassifier(max_depth=13, random_state=23) es 87.2%\n",
      "La precisión de DecisionTreeClassifier(max_depth=14, random_state=23) es 87.3%\n",
      "La precisión de DecisionTreeClassifier(max_depth=15, random_state=23) es 86.9%\n"
     ]
    }
   ],
   "source": [
    "# Obtener clasificador con mejor parámetro y acierto sobre test\n",
    "#for d in [2, 4, 8, 16, 32, 64]:\n",
    "#for d in [12, 13, 14, 15, 16, 17]:\n",
    "X = np.concatenate((X_train, X_val))\n",
    "Y = np.concatenate((y_train, y_val))\n",
    "for d in range(best - 2, best + 2):\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=23).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "    if acc > max:\n",
    "        best = d\n",
    "        max = acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 3**   \n",
    "\n",
    "Prueba otros parámetros del DecisionTreeClassifier que mejoren la tasa de acierto. En cualquier caso la selección de estos parámetros debe seguir el protocolo de experimentación antes expuesto. Esto es, escoger el mejor parámetro con datos de validación y reportar resultados con los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de DecisionTreeClassifier(max_depth=12, random_state=23) es 86.9%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=12, random_state=23) es 88.0%\n",
      "La precisión de DecisionTreeClassifier(criterion='log_loss', max_depth=12, random_state=23) es 88.0%\n",
      "La precisión de DecisionTreeClassifier(max_depth=13, random_state=23) es 87.2%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=13, random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(criterion='log_loss', max_depth=13, random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(max_depth=14, random_state=23) es 87.3%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(criterion='log_loss', max_depth=14, random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(max_depth=15, random_state=23) es 86.9%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=15, random_state=23) es 88.4%\n",
      "La precisión de DecisionTreeClassifier(criterion='log_loss', max_depth=15, random_state=23) es 88.4%\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_val))\n",
    "Y = np.concatenate((y_train, y_val))\n",
    "bestCri = \"gini\"\n",
    "for d in range(best - 2, best + 2):\n",
    "    for c in [\"gini\", \"entropy\", \"log_loss\"]:\n",
    "        clf = DecisionTreeClassifier(max_depth=d, criterion=c, random_state=23).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "        if acc > max:\n",
    "            best = d\n",
    "            max = acc\n",
    "            bestCri = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=11, min_samples_split=4,\n",
      "                       random_state=23) es 87.8%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=11, min_samples_split=5,\n",
      "                       random_state=23) es 87.8%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=11, min_samples_split=6,\n",
      "                       random_state=23) es 87.7%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=11, min_samples_split=7,\n",
      "                       random_state=23) es 87.8%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_split=4,\n",
      "                       random_state=23) es 88.2%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_split=5,\n",
      "                       random_state=23) es 88.2%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_split=6,\n",
      "                       random_state=23) es 88.1%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=12, min_samples_split=7,\n",
      "                       random_state=23) es 88.0%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_split=4,\n",
      "                       random_state=23) es 88.4%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_split=5,\n",
      "                       random_state=23) es 88.3%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_split=6,\n",
      "                       random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_split=7,\n",
      "                       random_state=23) es 88.3%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=4,\n",
      "                       random_state=23) es 88.5%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=5,\n",
      "                       random_state=23) es 88.6%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=6,\n",
      "                       random_state=23) es 88.4%\n",
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=7,\n",
      "                       random_state=23) es 88.5%\n"
     ]
    }
   ],
   "source": [
    "for d in range(best - 2, best + 2):\n",
    "    for l in [4, 5, 6, 7]:\n",
    "        clf = DecisionTreeClassifier(max_depth=d, criterion=\"entropy\", min_samples_split=l, random_state=23).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "        if acc > max:\n",
    "            best = d\n",
    "            max = acc\n",
    "            bestCri = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de DecisionTreeClassifier(criterion='entropy', max_depth=14, min_samples_split=5,\n",
      "                       random_state=23) es 88.1%\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=d, criterion=\"entropy\", min_samples_split=5, random_state=23).fit(X, Y)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
