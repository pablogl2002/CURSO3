{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST con K-vecinos más vercanos\n",
    "En esta segunda sesión práctica vamos a entrenar clasificadores basados en distancias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)\n",
    "\n",
    "data = mnist.data\n",
    "targets = mnist.target \n",
    "\n",
    "targets=targets.to_numpy()\n",
    "targets=np.int8(targets)\n",
    "\n",
    "data=data.to_numpy()\n",
    "data=np.float32(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos\n",
    "\n",
    "Vamos a partir los datos en tres conjuntos: training, validation y test. Con un 80%, 10% y 10% respectivamente. \n",
    "\n",
    "Emplearemos el conjunto de training para aprender los parámetros del modelos, el conjunto de validation para escoger los mejores hiperparámetros. Finalmente reportaremos el resultado final sobre el conjunto de test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1**    \n",
    "\n",
    "Realiza la partición de los datos en las particiones definidas (80%,10%,10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de los datos\n",
    "tam_test = 0.1\n",
    "tam_val = 0.1\n",
    "tam_train = 0.8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=tam_test, random_state=23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=tam_val/(tam_train + tam_test), random_state=23) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 2**   \n",
    "\n",
    "Prueba un clasificador KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prueba el clasificador KNeighborsClassifier\n",
    "clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 3**   \n",
    "\n",
    "Encuentra el mejor valor de \"K\" con el conjunto de validación y obtén el resultados para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimar el mejor parámetro K con el conjunto de validación\n",
    "best = -1\n",
    "bestK = -1\n",
    "\n",
    "for k in [1,2,3,4,5]:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        bestK = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtener tasa de acierto con el conjunto de test para el mejor K\n",
    "X = np.concatenate((X_train, X_val))\n",
    "Y = np.concatenate((y_train, y_val))\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=bestK).fit(X, Y)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 4**\n",
    "\n",
    "Cambia la distancia que se emplea (por defecto la L2) por la L1. Ver la documentación para ver cómo poder definir funciones de distancia y pasarlas como parámetro. Esto ralentiza bastante la clasificación pero se puede emplear el parámetro \"n_jobs\" para paralelizar el proceso. Aún así el resultado es muy lento. Podríamos concluir que sk-learn no es una librería adecuada para clasificar con KNN con métricas definidas por el usuario. Por ejemplo no permite definir un algoritmo rápido de tipo kd_tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modifica la función de distancia\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, p=1, n_jobs=-1).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "'''\n",
    "X = np.concatenate((X_train, X_val))\n",
    "Y = np.concatenate((y_train, y_val))\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, p=1, n_jobs=-1).fit(X, Y)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5**\n",
    "\n",
    "Encuentra el mejor valor de \"K\" y la mejor proyección a PCA con el conjunto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = -1\n",
    "bestNc = -1\n",
    "bestK = -1\n",
    "\n",
    "for nc in [2, 4, 8, 16, 32]:\n",
    "    for k in [1,2,3,4,5]:\n",
    "        clf = make_pipeline(PCA(n_components=nc), KNeighborsClassifier(n_neighbors=k, p=1, n_jobs=-1)).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))\n",
    "\n",
    "        if acc>max:\n",
    "            max = acc\n",
    "            bestNc = nc\n",
    "            bestK = k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midist(x, y):\n",
    "    return np.sum(np.abs(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midist(np.array([1,2,3]),np.array([4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3, metric=midist, n_jobs=-1).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de KNeighborsClassifier(metric=<function midist at 0x7f526d8c9120>, n_jobs=-1,\n",
      "                     n_neighbors=3) es 95.0%\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3, metric=midist, n_jobs=-1).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_val[:100], clf.predict(X_val[:100,:])[:100])\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
