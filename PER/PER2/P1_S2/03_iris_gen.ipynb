{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris con clasificadores generativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que los árboles de decisión, es muy fácil aprender y evaluar clasificadores generativos con sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura del corpus iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, shuffle=True, random_state=23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador naive Bayes Gaussiano es muy fácil de ajustar ya que, básicamente, basta hallar la media y varianza empíricas por cada par clase-dimensión separadamente. No obstante, en la práctica solemos encontrarnos con varianzas empíricas (quasi-)nulas para algunos pares clase-dimensión que ocasionan inestabilidad en los cálculos. Para reducir esta inestabildad se añade una porción de la mayor varianza empírica hallada a todas las varianzas empíricas. Esta porción se especifica mediante el parámetro var_smoothing (1e-9 por omisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de GaussianNB() es 93.3%\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB(var_smoothing=1e-9).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Estudia el efecto de var_smoothing sobre la precisión de naive Bayes Gaussiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de GaussianNB() es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=1e-08) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=1e-07) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=1e-06) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=1e-05) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=0.0001) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=0.001) es 93.3%\n",
      "La precisión de GaussianNB(var_smoothing=0.01) es 95.0%\n",
      "La precisión de GaussianNB(var_smoothing=0.1) es 95.0%\n",
      "La precisión de GaussianNB(var_smoothing=1) es 91.7%\n",
      "La precisión de GaussianNB(var_smoothing=10) es 70.0%\n"
     ]
    }
   ],
   "source": [
    "for vs in [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10]:\n",
    "    clf = GaussianNB(var_smoothing=vs).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA ajusta una Gaussiana a cada clase asumiendo que todas las clases comparten la misma matriz de covarianzas. Al igual que la estimación de varianzas empíricas en naive Bayes Gaussiano, la estimación de la matriz de covarianzas en LDA se regulariza apropiadamente. En concreto, la técnica de regularización por omisión emplea un parámetro de tolerancia, tol (1e-4 por omisión), que controla su fuerza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de LinearDiscriminantAnalysis() es 98.3%\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis(tol=1e-4).fit(X_train, y_train)\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Estudia el efecto de tol sobre la precisión de LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de LinearDiscriminantAnalysis(tol=1e-09) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=1e-08) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=1e-07) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=1e-06) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=1e-05) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis() es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=0.001) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=0.01) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=0.1) es 98.3%\n",
      "La precisión de LinearDiscriminantAnalysis(tol=1) es 30.0%\n"
     ]
    }
   ],
   "source": [
    "for t in [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1]:\n",
    "    clf = LinearDiscriminantAnalysis(tol=t).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA ajusta una Gaussiana independiente a cada clase. Emplea el mismo parámetro de tolerancia que LDA para regularizar la estimación de matrices de covarianzas, aunque en este caso no afecta a las predicciones, sino solo a los avisos que se producen cuando se considera que las muestras son colineales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de QuadraticDiscriminantAnalysis() es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(tol=0.5) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(tol=0.9) es 98.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "for tol in [1e-4, .5, .9]:\n",
    "    clf = QuadraticDiscriminantAnalysis(tol=tol).fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regularización en QDA se lleva a cabo mediante el parámetro reg_param (0.0 por omisión). QDA interpola la matriz de covarianzas empírica con la identidad en función de reg_param, de 0.0 (matriz empírica) a 1.0 (identidad)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Estudia el efecto de reg_param sobre la precisión de QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-09) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-09, tol=0.5) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-09, tol=0.9) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-06) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-06, tol=0.5) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1e-06, tol=0.9) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.001) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.001, tol=0.5) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.001, tol=0.9) es 98.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.1) es 95.0%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.1, tol=0.5) es 95.0%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=0.1, tol=0.9) es 95.0%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1) es 93.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1, tol=0.5) es 93.3%\n",
      "La precisión de QuadraticDiscriminantAnalysis(reg_param=1, tol=0.9) es 93.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\pablo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "for rp in [1e-9, 1e-6, 1e-3, 0.1,1]:\n",
    "    for tol in [1e-4, .5, .9]:\n",
    "        clf = QuadraticDiscriminantAnalysis(tol=tol,reg_param=rp).fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "        print('La precisión de {0!s} es {1:.1%}'.format(clf, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
